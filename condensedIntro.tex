\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{SPROJ Intro}
\author{Derek Low}
\date{2017-2018}

\usepackage{natbib}
\usepackage{graphicx}
\renewcommand{\baselinestretch}{1.6}
\begin{document}

\maketitle

\section{Introduction}
\tab The studies of biology and machine learning have historically intertwined, with concepts from each field being applied to the other. One prominently applied machine learning system, the neural network (NN), emerged from neuroscientific inspiration and research (McCullock and Pitts, 1943; Marblestone et al., 2016). Neural networks have been further researched with biological plausibility in mind, such as the research in spike timing dependent plasticity as a method of backpropagation (Marblestone et al., 2016). Genetic algorithms, another machine learning method, emerged from studies of the ecological phenomenon of evolution (Mitchell, 1996). Machine learning methods are currently applied to neuroscientific research, such as the development of  Latent Factor Analysis via Dynamical Systems (LFADS) to predict behavioral variables and model the neural population activity of monkey and human brains, based on single trials of activity (Pandarinath et al., 2017).\par
\tab Computational neuroscientific research extends outside the domain of machine learning neural network configurations, with other methods of from various fields applied to studies of biological neural network configuration (i.e. configurations of live neurons studied \textit{in vitro}). One common focus of these methods is the focus on understanding neuron connectivity. In particular, information theory research has been applied to research of neuron configurations and connections via an algorithm based on Transfer Entropy (Stetter et al., 2012). The Granger Causality (GC) statistical analysis method puruses a similar goal of understanding underlying network configurations by relating the GC-derived connectivity to the structural connectivity of simulated neuron networks (Zhou et al., 2014). Mutual information, joint transfer entropy, cross-correlation methods have been similarly studied and compared towards similar goals (Garfalo et al., 2009). However, NN-based methods towards the reconstruction of neuron connectivity have not been thoroughly explored.\par
\tab The essential component that resulted in the major resurgence of neural networks in the 1980's is the development of error backpropagation. Backpropagation is a method in which a network is trained by first providing the network with an input, and generating and output, and a corresponding error for each output. Then, changes are applied to the network, starting from the output node and moving towards the input node, based on this error (Widrow and Lehr, 1990). The strength of backpropagation is in its ability to tune the weights of each specific node, allowing for more precise training to achieve lowest possible error values (Gurney, 1997).\par
\tab We aim to implement certain methods widely used in neural networks towards the reconstruction of connectivity in networks of neurons, with similar goals to the aforementioned research based in information theory and statistical analysis methods. To do this, we simulate a pre-determined network of neurons to produce a spike-time matrix for $n$ neurons, and predict the activity of neurons in the next time step via an activation function and a weight matrix detailing the connections between all neurons in the network. We calculate error between the predicted output of the system and the actual output provided in the next time step of the spike-time matrix, and propagate that error through matrix W.\par
\end{document}